# -*- coding: utf-8 -*-
"""First_code_draft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SltpKtb0Nu7KWkKG3TySXi49WFAXKx2L
"""

# Import necessary libraries
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt

# mounting google drive
from google.colab import drive
drive.mount('/content/drive')

"""Once your Drive is mounted, you can load your data. Here's an example of how to load a CSV file named `your_data.csv` located in the root of your Google Drive. Please adjust the path and filename as needed."""

import pandas as pd

# Replace 'your_data.csv' with the actual path to your file in Google Drive
data_path = '/content/drive/MyDrive/U.C/I.T_PROJECT/Datasets/Checkouts_By_Title_Data_Lens_2005.csv'

try:
    df = pd.read_csv(data_path)
    print("Data loaded successfully!")
    display(df.head(5))
except FileNotFoundError:
    print(f"Error: The file '{data_path}' was not found. Please check the path and filename.")
except Exception as e:
    print(f"An error occurred: {e}")

# === DATA CLEANING: Standardize and sanitize all input ===

# 1. Standardize column names: lowers case, strips whitespace, replaces any internal spaces with underscores.
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

# 2. Remove exact duplicate rows across all columns, ensuring no redundant entries from data entry errors.
df = df.drop_duplicates()

# 3. Drop empty rows where every field is NA/missing value.
df = df.dropna(how='all')

df.head(5)

# 4. Keep only rows where key columns exist. Here, you require 'bibnumber' (unique item ID) and 'checkoutdatetime' (date checked out).
essential_cols = ['bibnumber', 'checkoutdatetime']
for col in essential_cols:
    if col in df.columns:
        df = df[df[col].notnull()]  # only keep rows with non-null essentials

df.head(5)

# 5. Parse checkoutdatetime as date; create a new column with the parsed dates. pd.to_datetime infers date formats and coerces invalid to NaT.
df['checkoutdatetime_parsed'] = pd.to_datetime(df['checkoutdatetime'], errors='coerce')

df.head(5)

# 6. Remove entries with missing or non-realistic dates (filtered to years >= 1900 and dates up to today).
df = df[df['checkoutdatetime_parsed'].notnull()]
df = df[df['checkoutdatetime_parsed'] >= pd.Timestamp('1900-01-01')]
df = df[df['checkoutdatetime_parsed'] <= pd.Timestamp.today()]

# This optional block lets you do deeper validation on columns like barcodes (e.g., minimum length)
# if 'itembarcode' in df.columns:
#     df = df[df['itembarcode'].astype(str).str.len() >= 5]

print("Data cleaned. Shape:", df.shape)
print("Columns:", df.columns.tolist())
print("\nSample rows:")
display(df.head())

# === FEATURE ENGINEERING AND AGGREGATION ===

# 7. Create 'year_month' field from parsed datetime for aggregation by month.
df['year_month'] = df['checkoutdatetime_parsed'].dt.to_period('M').astype(str)
# (This allows you to group checkouts by actual calendar month and track trends over time.)
df.head(5)

# 8. AGGREGATION: Group by 'bibnumber' (unique item ID), 'year_month', and 'itemtype' for monthly counts.
# This produces a compact table where each row is a specific item's usage for one month, possibly split by type.
group = df.groupby(['bibnumber', 'year_month', 'itemtype']).size().reset_index(name='checkouts')
group.head(5)

# 9. Extract separate numeric year and month as additional features for modeling temporal effects.
group['year'] = pd.to_datetime(group['year_month'] + '-01').dt.year
group['month'] = pd.to_datetime(group['year_month'] + '-01').dt.month
group.head(5)

# 10. Prepare model features: 'year', 'month', and 'itemtype' (categorical).
# No 'title' or content-based features exist in your source, so aggregation is strictly by bibnumber, month, and type.
features = ['year', 'month', 'itemtype']
X = group[features].copy()
y = group['checkouts']

X.head(5)

y.head(5)

# === MODEL TRAINING SETUP ===

# 11. Split to training (80%) and testing (20%) randomly for robust evaluation.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True)

# 12. Preprocessing configuration:
num_cols = ['year', 'month']         # Numerical features for imputation (fill missing with median)
cat_cols = ['itemtype']              # Categorical feature for one-hot encoding

numeric_transformer = SimpleImputer(strategy='median')  # Fill missing numerics with median

categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),  # Fill categoricals with 'unknown'
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))        # Convert all categorical types to one-hot
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, num_cols),
    ('cat', categorical_transformer, cat_cols)
])

numeric_transformer

categorical_transformer

preprocessor

# 13. Fit preprocessing pipeline to training data, then transform both train and test
preprocessor.fit(X_train)
X_train_p = preprocessor.transform(X_train)
X_test_p = preprocessor.transform(X_test)

X_train_p

X_test_p

# === MODELING AND EVALUATION ===

# 14. Train a Random Forest model on processed features
rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_train_p, y_train)
rf_preds = rf.predict(X_test_p)

rf

rf_preds

# 15. Calculate evaluation metrics: Mean Absolute Error, Root Mean Squared Error, R-Squared
rf_mae = mean_absolute_error(y_test, rf_preds)
rf_mse = mean_squared_error(y_test, rf_preds)
rf_rmse = np.sqrt(rf_mse)
rf_r2 = r2_score(y_test, rf_preds)

print("\nRandom Forest performance:")
print(f" MAE: {rf_mae:.4f}  RMSE: {rf_rmse:.4f}  R2: {rf_r2:.4f}")

# 16. Train a neural network (MLP) for comparison
mlp = MLPRegressor(hidden_layer_sizes=(64,32), max_iter=500, random_state=42)
try:
    mlp.fit(X_train_p, y_train)
    mlp_preds = mlp.predict(X_test_p)
    mlp_mae = mean_absolute_error(y_test, mlp_preds)
    mlp_rmse = np.sqrt(mean_squared_error(y_test, mlp_preds))
    mlp_r2 = r2_score(y_test, mlp_preds)
    print("\nMLPRegressor performance:")
    print(f" MAE: {mlp_mae:.4f}  RMSE: {mlp_rmse:.4f}  R2: {mlp_r2:.4f}")
except Exception as e:
    print("\nMLP training failed or did not converge within iterations:", e)
    mlp_preds = None

mlp

# === VISUALIZATION AND INTERPRETATION ===

# 17. Create a new figure for plotting the total monthly checkouts for all items combined
plt.figure(figsize=(10,4))  # Set figure size for readability
# Aggregate the total checkouts per month across the entire dataset
monthly_total = group.groupby('year_month')['checkouts'].sum().reset_index()
# Plot the checkouts over time as a line chart (trend visualization)
plt.plot(monthly_total['year_month'], monthly_total['checkouts'])
plt.title("Monthly Total Checkouts")  # Chart title for context
plt.xlabel("Year-Month")              # X-axis label
plt.ylabel("Total Checkouts")         # Y-axis label
plt.xticks(rotation=45)               # Rotate x labels for better readability
plt.tight_layout()                    # Adjust layout so axes/titles don't overlap
plt.show()                            # Display the plot

# 18. Bar plot for most checked out item types (top 10), helps spot most used formats
if 'itemtype' in group.columns:  # Ensure the column exists in the aggregated data
    # Compute total checkouts by item type and take the top 10 most checked out types
    top_types = group.groupby('itemtype')['checkouts'].sum().sort_values(ascending=False).head(10)
    plt.figure(figsize=(8,4))    # Slightly smaller figure for bar chart
    # Create a bar plot to show which item types (e.g. books, DVDs) circulate the most
    plt.bar(top_types.index, top_types.values)
    plt.title("Top Item Types by Total Checkouts")    # Chart title for context
    plt.xlabel("Item Type")                           # X-axis: the type/format of item
    plt.ylabel("Total Checkouts")                     # Y-axis: sum of checkouts for each type
    plt.xticks(rotation=45, ha='right')               # Rotate x labels for clarity
    plt.tight_layout()                                # Prevent label overlap
    plt.show()                        # Show the bar plot

# 19. Scatterplot for model calibration: prediction vs reality for the Random Forest model
plt.figure(figsize=(6,6))             # Square plot for parity/accuracy
# Plot actual checkouts (y_test) vs. predicted checkouts (rf_preds) as scatter points
plt.scatter(y_test, rf_preds, alpha=0.6)
# Compute min/max of both axes to set a 45-degree reference "perfect prediction" line
minv, maxv = min(y_test.min(), rf_preds.min()), max(y_test.max(), rf_preds.max())
plt.plot([minv, maxv], [minv, maxv])  # Reference line: where prediction equals true values
plt.xlabel("Actual checkouts")        # X-axis: ground truth
plt.ylabel("Predicted checkouts (RF)")# Y-axis: model prediction
plt.title("Random Forest: Actual vs Predicted")       # Title for context
plt.tight_layout()                                   # Layout adjustment
plt.show()                                           # View calibration plot

# 20. Permutation feature importance: visualize which features most influence Random Forest predictions
try:
    # Compute importances by how much model error increases when each feature is permuted/disrupted
    perm = permutation_importance(rf, X_test_p, y_test, n_repeats=10, random_state=42, n_jobs=-1)
    num_features = num_cols  # List of numerical features
    # Get names of one-hot encoded categorical features after preprocessing
    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']
    ohe_names = list(ohe.get_feature_names_out(cat_cols))
    feature_names = list(num_features) + ohe_names  # Final feature name list in order
    # Build a pandas Series of average importances, sort and take top 15 for interpretability
    fi = pd.Series(perm.importances_mean, index=feature_names).sort_values(ascending=False).head(15)
    print("\nTop permutation importances (RF):")  # Display top features in output
    print(fi)
    plt.figure(figsize=(8,4))                  # Figure for importance bar chart
    plt.bar(fi.index, fi.values)               # Feature names vs. importance scores
    plt.xticks(rotation=45, ha='right')        # Rotate and align x labels
    plt.title("Permutation Feature Importances (RF)")    # Title for interpretation
    plt.tight_layout()                         # Prevent label overlap
    plt.show()                                 # Show bar plot
except Exception as e:
    # Handle possible permutation errors gracefully for logging/debug
    print("Permutation importance failed:", e)

ohe_names

feature_names

# === SAVE ALL OUTPUTS FOR AUDIT AND SHARING ===

# 21. Output cleaned/aggregated data and model metrics as CSVs for sharing, reproducibility, or audit trail
# out_dir = "/mnt/data/i_t_project_outputs"  # Output directory
out_dir = "/content/drive/My Drive/U.C/I.T_PROJECT/Output_Dataset"
os.makedirs(out_dir, exist_ok=True)        # Ensure output directory exists
# Save per-item/month checkouts and features for traceability and future use
group.to_csv(os.path.join(out_dir, "monthly_aggregated_checkouts.csv"), index=False)
# Prepare summary of model performance metrics
metrics = {
    'model': ['RandomForest', 'MLP'],
    'MAE': [rf_mae, (mlp_mae if mlp_preds is not None else None)],
    'RMSE': [rf_rmse, (mlp_rmse if mlp_preds is not None else None)],
    'R2': [rf_r2, (mlp_r2 if mlp_preds is not None else None)]
}
metrics_df = pd.DataFrame(metrics)
# Save metrics as CSV for reporting/comparison/audit
metrics_df.to_csv(os.path.join(out_dir, "model_performance_summary.csv"), index=False)
print("\nSaved outputs to:", out_dir)          # Confirm output save location and dataset used
print("Primary dataset path used:", data_path)

metrics_df

