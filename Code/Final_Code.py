# -*- coding: utf-8 -*-
"""Second_code_draft_with_time_series_split.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b7vzyNGWtNnULQeEIcik_NLq3-FHKSO-
"""

# === WHY THIS MATTERS: SETTING UP THE PROJECT ENVIRONMENT ===
# Technical: These imports enable you to process data, build models, save results, and create visuals.
# Layman: These are the Python tools that let us read spreadsheet data, make predictions, and create easy-to-share charts.

import os  # For handling file locations and folders
import pandas as pd  # For spreadsheet/table data
import numpy as np  # For mathematical and statistical calculations
import joblib  # For saving and loading trained models—makes work reusable

# Machine learning tools for preparing and training predictive models
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.ensemble import HistGradientBoostingRegressor  # Advanced model for numerical prediction
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # To measure accuracy
from sklearn.impute import SimpleImputer  # Fill in blanks automatically
from sklearn.preprocessing import OneHotEncoder, StandardScaler  # Convert text to numbers, normalize values
from sklearn.compose import ColumnTransformer  # Apply preprocessing to relevant columns
from sklearn.pipeline import Pipeline  # Link all preparation/model steps in order
import matplotlib.pyplot as plt  # Make charts and save as images

# mounting google drive
from google.colab import drive
drive.mount('/content/drive')

# ---------- WHY THIS SECTION MATTERS: CONFIGURING FILE LOCATIONS AND TIME SPLITS ----------
# Technical: Ensures your results are repeatable by anyone and files are stored accessibly.
# Layman: Sets where we get our library data and where to save final results. Also makes sure we're evaluating on realistic "future" checkouts.

DATA_PATH = '/content/drive/My Drive/U.C/I.T_PROJECT/Datasets/Checkouts_By_Title_Data_Lens_2005.csv'
OUT_DIR = '/content/drive/My Drive/U.C/I.T_PROJECT/Output_Dataset_improved'
RANDOM_STATE = 42     # Makes random choices consistent (important for experiments and sharing results)
TEST_MONTH_SPLIT = '2017-01'   # Train on older months, test on newer months—like predicting future use

# Make sure the output folder exists so results won't get lost
os.makedirs(OUT_DIR, exist_ok=True)

# === SECTION: DATA LOADING AND CLEANING ===
# Why this matters:
# Technical: Bad or missing data can ruin analysis! Cleaning ensures accuracy and reliability.
# Layman: Removes errors, repeats, or blanks so we're only working with real, usable checkouts.

df = pd.read_csv(DATA_PATH)
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")  # Standardize headers for code
df = df.drop_duplicates()        # Remove repeated rows
df = df.dropna(how='all')        # Remove empty records

df.head(5)

# Check we have all critical columns, stop if anything's missing
required = ['bibnumber', 'checkoutdatetime', 'itemtype']
for c in required:
    if c not in df.columns:
        raise ValueError(f"Required column missing: {c}")

# Convert the 'checkoutdatetime' column (text date/timestamp) into a real datetime object.
# Technical: This function parses the string into a standardized date format Python can understand.
# Why it matters: Ensures dates are in a usable format for filtering, grouping, and feature creation.
df['checkoutdatetime_parsed'] = pd.to_datetime(df['checkoutdatetime'], errors='coerce')

# Remove any rows where the date couldn't be interpreted (became NaT, Not-a-Time).
# Technical: Keeps only rows with valid, parseable dates.
# Why it matters: Stops invalid or blank dates from causing errors in later time-based analysis.
df = df[df['checkoutdatetime_parsed'].notna()]

# Filter to keep only dates from Jan 1, 1900 through today.
# Technical: Discards records with dates that are obviously incorrect (well before libraries were computerized, or from the future).
# Why it matters: Cleans out accidental typos or import errors, so that all remaining data is realistic and trustworthy for trend analysis.
df = df[(df['checkoutdatetime_parsed'] >= '1900-01-01') & (df['checkoutdatetime_parsed'] <= pd.Timestamp.today())]

df.head(5)

# === SECTION: AGGREGATION AND FEATURE ENGINEERING ===
# Why this matters:
# Technical: Aggregating lets us study patterns rather than isolated events.
# Layman: Instead of seeing every individual checkout, we combine them by month and item so we see useful trends.

df['year_month'] = df['checkoutdatetime_parsed'].dt.to_period('M').astype(str)

df.head(5)

group = df.groupby(['bibnumber', 'year_month', 'itemtype']).size().reset_index(name='checkouts')  # Monthly summary

group['date'] = pd.to_datetime(group['year_month'] + '-01')
group = group.sort_values(['bibnumber', 'date'])
group['year'] = group['date'].dt.year
group['month'] = group['date'].dt.month

group.head(5)

# Sin/Cos features: convert months to circles—so model can learn about cycles, like summer/winter patterns
group['month_sin'] = np.sin(2*np.pi*group['month']/12)
group['month_cos'] = np.cos(2*np.pi*group['month']/12)

group.head(5)

# Lag features: Show the model what happened previously—helpful for predicting "momentum"
group['checkouts_lag1'] = group.groupby('bibnumber')['checkouts'].shift(1).fillna(0)
group['checkouts_roll3'] = (
    group.groupby('bibnumber')['checkouts']
    .shift(1)
    .rolling(3, min_periods=1)
    .mean().reset_index(level=0, drop=True)
    .fillna(0)
)

group.head(5)

# === SECTION: MODEL PREPARATION AND TRAINING ===
# Why this matters:
# Technical: More features and proper splits create more accurate and future-proof predictions.
# Layman: Models learn from patterns in past months to "guess" future checkouts—using all trends, not just recent ups and downs.

feature_cols = ['year', 'month', 'month_sin', 'month_cos', 'itemtype', 'checkouts_lag1', 'checkouts_roll3']
X = group[feature_cols].copy()
y = group['checkouts'].copy()

X.head(5)

y.head(5)

# Time-based split: choose last 2 months as test
last_date = group['date'].max()
split_date = last_date - pd.DateOffset(months=2)  # last 2 months as test

train_idx = group['date'] < split_date
test_idx = ~train_idx

X_train = X.loc[train_idx].reset_index(drop=True)
X_test = X.loc[test_idx].reset_index(drop=True)
y_train = y.loc[train_idx].reset_index(drop=True)
y_test = y.loc[test_idx].reset_index(drop=True)

print("Split date:", split_date)
print("Train rows:", X_train.shape[0], "Test rows:", X_test.shape[0])

print("Train rows:", X_train.shape[0], "Test rows:", X_test.shape[0],TEST_MONTH_SPLIT)

# Log-transform stabilizes extremes (huge spikes/rare cases), making predictions more reliable
y_train_tr = np.log1p(y_train)

y_train_tr.head(5)

# === SECTION: DATA PREPROCESSING ===
# Why this matters:
# Technical: Models expect numbers not text, scaled values not random ranges. This step makes all data model-ready.
# Layman: Converts all our data into a format our prediction 'engine' understands—without losing important information.

num_cols = ['year', 'month', 'month_sin', 'month_cos', 'checkouts_lag1', 'checkouts_roll3']
cat_cols = ['itemtype']

numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
numeric_transformer

categorical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])
categorical_transformer

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, num_cols),
    ('cat', categorical_transformer, cat_cols)
], remainder='drop')
preprocessor

# === SECTION: MODELING PIPELINE ===
# Why this matters:
# Technical: Running preprocessing and prediction together ensures workflow is reproducible and reliable.
# Layman: Our "prediction engine" is now trained—it learns from all the past checkouts and their patterns to help forecast future demand.

model = HistGradientBoostingRegressor(random_state=RANDOM_STATE, max_iter=300)
model

pipe = Pipeline([
    ('pre', preprocessor),
    ('model', model)
])
pipe

pipe.fit(X_train, y_train_tr)

pred_log = pipe.predict(X_test)
pred = np.expm1(pred_log)  # Return to actual (not log-transformed) numbers

pred

# === SECTION: METRICS AND MODEL EVALUATION ===
# Why this matters:
# Technical: Metrics let us quantify model accuracy and error, crucial for tuning and trust.
# Layman: Tells us "How good are these predictions?"—a safety check before using the results.

def metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    return {'MAE': mae, 'RMSE': rmse, 'MSE': mse, 'R2': r2}

global_metrics = metrics(y_test, pred)
print("Global metrics:", global_metrics)

metrics_df = pd.DataFrame([{**global_metrics}])
metrics_df.to_csv(os.path.join(OUT_DIR, 'model_metrics_summary.csv'), index=False)

metrics_df

joblib.dump(pipe, os.path.join(OUT_DIR, 'pipeline_histgb.joblib'))  # Save the trained preprocessing + model pipeline to disk for later reuse

group.to_csv(os.path.join(OUT_DIR, 'monthly_aggregated_checkouts_with_features.csv'), index=False)

group

# Per-itemtype metrics: See which material formats the model predicts very well and which may need more attention
merge_test = X_test.copy()
merge_test['y_true'] = y_test.values
merge_test['y_pred'] = pred
merge_test

per_type = merge_test.groupby('itemtype').apply(lambda df: pd.Series(metrics(df['y_true'], df['y_pred']))).reset_index()
per_type.to_csv(os.path.join(OUT_DIR, 'metrics_by_itemtype.csv'), index=False)
per_type.head(5)

# === SECTION: MONTHLY VISUALIZATION ===
# Why this matters:
# Technical: Trend plots help with diagnostics, model validation, and understanding time dependency.
# Layman: This chart shows at a glance how library usage changes month by month—easy to see patterns, surges, or dips.

monthly_total = group.groupby('date')['checkouts'].sum().reset_index()

plt.figure(figsize=(12,4))  # Wide chart: easier to spot monthly ups and downs
plt.plot(monthly_total['date'], monthly_total['checkouts'])
plt.title("Monthly Total Checkouts")      # Clear title
plt.xlabel("Date")                        # Shows which month
plt.ylabel("Total checkouts")             # Shows number borrowed
plt.tight_layout()                        # Avoid label overlap
plt.show()

plt.savefig(os.path.join(OUT_DIR, 'monthly_total_checkouts.png'))
plt.close()
print(f"Saved monthly trend plot to {OUT_DIR}/monthly_total_checkouts.png")

# === SECTION: YEARLY VISUALIZATION ===
# Why this matters:
# Technical: Yearly plotting smooths out noise, aiding in long-term planning and audit.
# Layman: Provides management a simple chart for annual reports—shows big picture changes over years.

yearly_total = group.groupby('year')['checkouts'].sum().reset_index()

plt.figure(figsize=(8,4))
plt.bar(yearly_total['year'], yearly_total['checkouts'], color='skyblue')
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

type_rank = (
    group.groupby('itemtype')['checkouts']
         .sum()
         .sort_values(ascending=False)
         .reset_index()
)

top20 = type_rank.head(20)
top20.to_csv(os.path.join(OUT_DIR, 'itemtype_top20_most_used.csv'), index=False)

plt.figure(figsize=(10,6))
plt.barh(top20['itemtype'], top20['checkouts'], color='steelblue')
plt.gca().invert_yaxis()  # most-used at the top
plt.title("Top 20 Most Used Item Types")
plt.xlabel("Total checkouts")
plt.ylabel("Item type")
plt.tight_layout()
plt.show()
plt.savefig(os.path.join(OUT_DIR, 'itemtype_top20_most_used.png'))
plt.close()

bottom20 = type_rank.tail(20).iloc[::-1]  # smallest at top of chart
bottom20.to_csv(os.path.join(OUT_DIR, 'itemtype_top20_least_used.csv'), index=False)

plt.figure(figsize=(10,6))
plt.barh(bottom20['itemtype'], bottom20['checkouts'], color='indianred')
plt.gca().invert_yaxis()
plt.title("Top 20 Least Used Item Types")
plt.xlabel("Total checkouts")
plt.ylabel("Item type")
plt.tight_layout()
plt.show()
plt.savefig(os.path.join(OUT_DIR, 'itemtype_top20_least_used.png'))
plt.close()

# Final message for users of all backgrounds
print(f"Saved outputs to {OUT_DIR}")

